# MNQA (MultiNativQA)

### Paper

Title: NativQA: Multilingual Culturally-Aligned Natural Query for LLMs

Abstract:
Natural Question Answering (QA) datasets play a crucial role in evaluating the capabilities of large language models (LLMs), ensuring their effectiveness in real-world applications. Despite the numerous QA datasets that have been developed, there is a notable lack of region-specific datasets generated by native users in their own languages. This gap hinders the effective benchmarking of LLMs for regional and cultural specificities. Furthermore, it also limits the development of fine-tuned models. In this study, we propose a scalable, language-independent framework, NativQA, to seamlessly construct culturally and regionally aligned QA datasets in native languages, for LLM evaluation and tuning. We demonstrate the efficacy of the proposed framework by designing a multilingual natural QA dataset, \mnqa, consisting of ~64k manually annotated QA pairs in seven languages, ranging from high to extremely low resource, based on queries from native speakers from 9 regions covering 18 topics. We benchmark open- and closed-source LLMs with the MultiNativQA dataset. We also showcase the framework efficacy in constructing fine-tuning data especially for low-resource and dialectally-rich languages. We made both the framework NativQA and MultiNativQA dataset publicly available for the community.



Homepage: https://nativqa.gitlab.io/


### Citation

```
@article{hasan2024nativqa,
  title = {{NativQA}: Multilingual Culturally-Aligned Natural Query for LLMs},
  author = {Hasan, Md. Arid and Hasanain, Maram and Ahmad, Fatema and Laskar, Sahinur Rahman and Upadhyay, Sunaya and Sukhadia, Vrunda N and Kutlu, Mucahid and Chowdhury, Shammur Absar and Alam, Firoj},
  year = {2024},
  url = {https://arxiv.org/abs/2407.09823},
  publisher = {arXiv:2407.09823},
}
```

### Groups and Tasks

#### Groups

- `mnqa`: Overall results for all nine datasets across seven locations and seven languages.


#### Tasks

* `mnqa`: evaluates all datasets
* `mnqa_arabic_qa`: evaluates the task in Arabic dataset collected from Qatar.
* `mnqa_assamese_in`: evaluates the task in Assamese dataset collected from India.
* `mnqa_bangla_bd`: evaluates the task in Bangla dataset collected from Bangladesh.
* `mnqa_bangla_in`: evaluates the task in Bangla dataset collected from India.
* `mnqa_english_bd`: evaluates the task in English dataset collected from Bangladesh.
* `mnqa_english_qa`: evaluates the task in Bangla dataset collected from Qatar.
* `mnqa_hindi_in`: evaluates the task in Hindi dataset collected from India.
* `mnqa_nepali_np`: evaluates the task in Nepali dataset collected from Nepal.
* `mnqa_turkish_tr:` evaluates the task in Turkish dataset collected from Turkey.


### Checklist

[x] New dataset.
